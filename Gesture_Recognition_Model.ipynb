{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "08392a43"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from PIL import Image\n",
        "import random\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Conv3D, MaxPool3D, Flatten, Dense\n",
        "from keras.layers import Dropout, Input, BatchNormalization, Reshape, LSTM\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.models import Model\n",
        "import tensorflow\n",
        "import keras\n"
      ],
      "id": "08392a43"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1HzIq3qVQmv",
        "outputId": "7dc1d731-161f-490d-eea3-b0c67cc7b55a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "id": "E1HzIq3qVQmv"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sv-K14I8V1Tg"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tensorflow.data.experimental.AUTOTUNE"
      ],
      "id": "Sv-K14I8V1Tg"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d755c745",
        "outputId": "681c8457-2ac5-4f3d-b284-4d61715f893f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 Folder_Name   Gesture_Name  Gesture_Class\n",
            "658  WIN_20180907_16_38_23_Pro_Thumbs Up_new  Thumbs Up_new              4\n",
            "659  WIN_20180907_16_41_09_Pro_Thumbs Up_new  Thumbs Up_new              4\n",
            "660  WIN_20180907_16_42_05_Pro_Thumbs Up_new  Thumbs Up_new              4\n",
            "661  WIN_20180907_16_42_55_Pro_Thumbs Up_new  Thumbs Up_new              4\n",
            "662  WIN_20180907_16_43_39_Pro_Thumbs Up_new  Thumbs Up_new              4\n",
            "                                Folder_Name   Gesture_Name  Gesture_Class\n",
            "95  WIN_20180907_15_43_36_Pro_Thumbs Up_new  Thumbs Up_new              4\n",
            "96  WIN_20180907_15_52_05_Pro_Thumbs Up_new  Thumbs Up_new              4\n",
            "97  WIN_20180907_15_54_30_Pro_Thumbs Up_new  Thumbs Up_new              4\n",
            "98  WIN_20180907_16_10_59_Pro_Thumbs Up_new  Thumbs Up_new              4\n",
            "99  WIN_20180907_16_39_59_Pro_Thumbs Up_new  Thumbs Up_new              4\n"
          ]
        }
      ],
      "source": [
        "# We will traverse through Project_data and build 2 DataFrames each for training and validation with below 4 columns\n",
        "#  Image_File_name | Image_Folder_Name | Image/Gesture_Class_Name | Image/Gesture_Class_label/numerical\n",
        "\n",
        "# Let us read train.csv and val.csv to build -  folder_name | Gesture_Name | Gesture_Class_label first.\n",
        "\n",
        "ROOT_PATH = \"/content/drive/My Drive/Gesture_Assignment/Project_data\"\n",
        "TRAIN_MAPPING = ROOT_PATH + \"/train.csv\"\n",
        "VAL_MAPPING = ROOT_PATH + \"/val.csv\"\n",
        "#img_width, img_height = 180, 180\n",
        "batch_size = 32\n",
        "# we have 30 images, but we chose 24 below and we pick them randomly and sort to ensure temporal dependence\n",
        "num_samples = 28\n",
        "num_of_channels = 3\n",
        "num_of_classes = 5\n",
        "    \n",
        "    \n",
        "train_map_df = pd.read_csv(TRAIN_MAPPING,sep=\";\",header=None)\n",
        "val_map_df = pd.read_csv(VAL_MAPPING,sep=\";\",header=None)\n",
        "DF_COLS = [\"Folder_Name\", \"Gesture_Name\", \"Gesture_Class\"]\n",
        "\n",
        "train_map_df.columns = DF_COLS\n",
        "val_map_df.columns = DF_COLS\n",
        "\n",
        "print(train_map_df.tail())\n",
        "print(val_map_df.tail())\n"
      ],
      "id": "d755c745"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d51b2d40"
      },
      "outputs": [],
      "source": [
        "def create_mapping_dataframe(dir_name,input_df):\n",
        "    \n",
        "    '''\n",
        "    We call this function with 'train' and 'val' as dir_name i.e. directory names argument.\n",
        "    and input_df which has three columns so far. We navigate through each image folder and build data frame.\n",
        "    This data frame will have 4 columns indicated by df_columns. This dataframe which we return then becomes\n",
        "    master referrence throughout the program.\n",
        "    \n",
        "    Arguments : \n",
        "      directory_name - train and val \n",
        "      dataframe - created in earlier step with mapping of folder, gesture name and class \n",
        "    Returns :\n",
        "      dataframe - with image file names populated\n",
        "    '''\n",
        "    \n",
        "    train_image_mapping = []\n",
        "    df_columns = [\"Image_File\", \"Folder_Name\", \"Gesture_Name\", \"Gesture_Class\"]\n",
        "\n",
        "    for each_folder in input_df['Folder_Name'].values:\n",
        "    \n",
        "        e_gesture_name = input_df.loc[input_df['Folder_Name'] == each_folder ,'Gesture_Name'].values[0]\n",
        "        e_gesture_class = input_df.loc[input_df['Folder_Name'] == each_folder ,'Gesture_Class'].values[0]\n",
        "    \n",
        "        folder_full_path = ROOT_PATH + \"/\" + dir_name + \"/\"  + each_folder\n",
        "        for each_file in os.listdir(folder_full_path):\n",
        "            train_image_mapping.append((each_file, each_folder, e_gesture_name, e_gesture_class))\n",
        "            \n",
        "    return_df = pd.DataFrame(data=train_image_mapping, columns=df_columns)\n",
        "    \n",
        "    return return_df"
      ],
      "id": "d51b2d40"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "873fe617"
      },
      "outputs": [],
      "source": [
        "def convert_images_to_numpy(image_files, folder, ops_mode):\n",
        "    \n",
        "    '''\n",
        "    Given the image file names, folder strucutre resize, and convert into numpy array.\n",
        "    Arguments:\n",
        "      image_files - list of image files coming from some folder\n",
        "      folder - folder name after ROOT_PATH\n",
        "      ops_mode - train or val to indicate location of images\n",
        "    Returns:\n",
        "      imagelist in multidimensional numpy array \n",
        "    '''\n",
        "    \n",
        "    return_images = np.zeros((len(image_files),img_width,img_height,3))\n",
        "    folder_full_path = ROOT_PATH + \"/\" + ops_mode + \"/\"  + folder\n",
        "    \n",
        "    count = 0\n",
        "    for each_file in image_files: \n",
        "        img = Image.open(os.path.join(folder_full_path, each_file))\n",
        "        img = img.resize((img_width, img_height))\n",
        "        img = np.asarray(img)\n",
        "        img = img/255\n",
        "        img = np.round(img,3)\n",
        "        return_images[count] = img\n",
        "        count += 1\n",
        "    \n",
        "    return(return_images)"
      ],
      "id": "873fe617"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QkG2B5BshL2m"
      },
      "outputs": [],
      "source": [
        "def convert_all_images_to_numpy(df, ops_mode):\n",
        "    \n",
        "    '''\n",
        "    Given the image folder, folder strucutre resize, and convert into numpy array.\n",
        "    Arguments:\n",
        "      df - Dataframe with mapping of folders, image files \n",
        "      ops_mode - train or val to indicate location of images\n",
        "    Returns:\n",
        "      imagelist in dictionary in multidimensional numpy array \n",
        "    '''\n",
        "\n",
        "    grouped_df = df.groupby('Folder_Name')\n",
        "    folder_list = grouped_df['Folder_Name'].unique().values.flatten()\n",
        "    \n",
        "    image_dict = {}\n",
        "\n",
        "    count = 0\n",
        "    for folders in folder_list:\n",
        "        each_folder = folders[0]\n",
        "        folder_full_path = ROOT_PATH + \"/\" + ops_mode + \"/\"  + each_folder\n",
        "        file_list_df = grouped_df.get_group(each_folder)['Image_File']\n",
        "        file_list_tuple = list(zip(file_list_df.index.values, file_list_df.values))\n",
        "        for each_tuple in file_list_tuple:\n",
        "            img = Image.open(os.path.join(folder_full_path, each_tuple[1]))\n",
        "            img = img.resize((img_width, img_height))\n",
        "            img = np.asarray(img)\n",
        "            img = img/255\n",
        "            img = np.round(img,3)\n",
        "            image_dict[each_tuple[0]] = img\n",
        "        print(\"Finished processing of folder {}\".format(count))\n",
        "        count += 1\n",
        "\n",
        "    return image_dict"
      ],
      "id": "QkG2B5BshL2m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_42JMId8vmr0"
      },
      "source": [],
      "id": "_42JMId8vmr0"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Bl6swceO8TZG"
      },
      "outputs": [],
      "source": [
        "def convert_images_to_numpy_from_dict(image_files, image_dict, df):\n",
        "    \n",
        "    '''\n",
        "    Given the image file names, folder strucutre resize, and convert into numpy array.\n",
        "    Arguments:\n",
        "      image_files - list of image files coming from some folder\n",
        "      folder - folder name after ROOT_PATH\n",
        "      ops_mode - train or val to indicate location of images\n",
        "    Returns:\n",
        "      imagelist in multidimensional numpy array \n",
        "    '''\n",
        "    grouped_df = df.groupby('Image_File')\n",
        "\n",
        "    return_images = np.zeros((len(image_files),img_width,img_height,3))\n",
        "    count = 0 \n",
        "    for each_file in image_files:\n",
        "        file_idx = grouped_df.get_group(each_file).index.values[0]\n",
        "        file_idx = str(file_idx)\n",
        "        #print(file_idx , type(file_idx))\n",
        "        #x = image_dict[file_idx]\n",
        "        return_images[count] = image_dict[file_idx]\n",
        "    \n",
        "    return(return_images)"
      ],
      "id": "Bl6swceO8TZG"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0722c811"
      },
      "outputs": [],
      "source": [
        "def batch_generator(df, batch_size, ops_mode, image_dict):\n",
        "    \n",
        "    '''\n",
        "    Generate the batch yielding X and y for training via CNN and LSTM.\n",
        "    Arguments:\n",
        "      df - dataframe with image_names , and other details like folder names , mapping to gestures etc.\n",
        "      batch_size - int value indicating how much we want to fit into one batch\n",
        "      ops_mode - train or val to indicate if we are calling generator for training or validation.\n",
        "        this operation will change folder structure to refer to \n",
        "    Returns:\n",
        "      Yields batch with X and y values\n",
        "    '''\n",
        "    \n",
        "\n",
        "    batch_array_size_tuple = (batch_size, num_samples, img_width, img_height, num_of_channels)\n",
        "\n",
        "    while True:\n",
        "        \n",
        "        # Let us shuffle DF to get different sequence of images in every batch\n",
        "        shuffled_df = df.groupby('Folder_Name', group_keys=False).apply(lambda x: x.sample(frac=1))\n",
        "        folder_list = shuffled_df['Folder_Name'].unique()\n",
        "        \n",
        "        # first for loop below creates batchs and also ensures any residual rows are placed in the last element of array\n",
        "        # batch X values are of the shape [ batch_size, num_of_samples_per_folder, image_width, image_height, channels ]\n",
        "        # Batch y values are of the shape [ batch_size, class_labels_in_one_hot ]\n",
        "        \n",
        "        folder_batches = []\n",
        "        for i in range(0, len(folder_list), batch_size):\n",
        "            folder_batches.append(folder_list[i:i+batch_size])\n",
        "        \n",
        "        # Let us iterate through batches and yield X and y for each batch\n",
        "        for each_batch in folder_batches:\n",
        "            batch_data = np.zeros(batch_array_size_tuple)\n",
        "            batch_labels = np.zeros((batch_size,5))\n",
        "\n",
        "            # we ignore first and last 2 frames in sequences thinking it may not have captured anything meaningful                        \n",
        "            #random_imglist = random.sample(range(0, 30), num_samples)\n",
        "            #random_imglist = sorted(random_imglist)\n",
        "            # we ignore first seueqnce thinking it may not have captured anything meaningful \n",
        "            random_imglist = range(1,num_samples+1)\n",
        "            \n",
        "            folder_count = 0\n",
        "            for each_folder in each_batch:\n",
        "                img_files = sorted(shuffled_df.groupby('Folder_Name')\n",
        "                                   .get_group(each_folder)[[\"Image_File\"]]\n",
        "                                   .values.flatten()\n",
        "                                  )\n",
        "                img_files = [img_files[i] for i in random_imglist]\n",
        "                #temp_data = convert_images_to_numpy(img_files, each_folder, ops_mode)\n",
        "                temp_data = convert_images_to_numpy_from_dict(img_files, image_dict, df)\n",
        "                batch_data[folder_count] = temp_data\n",
        "                gesture = int(shuffled_df.groupby('Folder_Name')\n",
        "                              .get_group(each_folder)['Gesture_Class'].values[0]\n",
        "                             )\n",
        "                batch_labels[folder_count, gesture] = 1\n",
        "                folder_count += 1\n",
        "\n",
        "            #print(\"Completed batch {}\".format(batch_data.shape))\n",
        "            yield batch_data, batch_labels\n"
      ],
      "id": "0722c811"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "999fa9f3"
      },
      "outputs": [],
      "source": [
        "# we create master mapping and store it in dataframe\n",
        "new_train_map_df = create_mapping_dataframe('train', train_map_df)\n",
        "new_val_map_df = create_mapping_dataframe('val', val_map_df)\n",
        "\n",
        "train_image_dict_file = ROOT_PATH + \"/\" + \"train_dict_2.npz\"\n",
        "val_image_dict_file = ROOT_PATH + \"/\" + \"val_dict_2.npz\"\n",
        "\n",
        "train_image_dict = np.load(train_image_dict_file)\n",
        "val_image_dict = np.load(val_image_dict_file)"
      ],
      "id": "999fa9f3"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4B9b6LkXCFA",
        "outputId": "14709537-7c2e-4188-e6ce-bc4f91e8c3da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.471 0.463 0.439]\n",
            "  [0.467 0.463 0.424]\n",
            "  [0.467 0.467 0.408]\n",
            "  ...\n",
            "  [0.541 0.522 0.475]\n",
            "  [0.525 0.529 0.475]\n",
            "  [0.518 0.518 0.471]]\n",
            "\n",
            " [[0.471 0.475 0.447]\n",
            "  [0.471 0.475 0.439]\n",
            "  [0.471 0.475 0.42 ]\n",
            "  ...\n",
            "  [0.533 0.518 0.475]\n",
            "  [0.522 0.518 0.467]\n",
            "  [0.51  0.514 0.463]]\n",
            "\n",
            " [[0.51  0.514 0.482]\n",
            "  [0.51  0.514 0.478]\n",
            "  [0.51  0.514 0.467]\n",
            "  ...\n",
            "  [0.529 0.51  0.467]\n",
            "  [0.518 0.514 0.463]\n",
            "  [0.506 0.514 0.463]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.506 0.439 0.4  ]\n",
            "  [0.341 0.259 0.235]\n",
            "  [0.345 0.243 0.212]\n",
            "  ...\n",
            "  [0.282 0.204 0.196]\n",
            "  [0.271 0.2   0.184]\n",
            "  [0.235 0.188 0.173]]\n",
            "\n",
            " [[0.341 0.286 0.251]\n",
            "  [0.129 0.075 0.067]\n",
            "  [0.176 0.106 0.102]\n",
            "  ...\n",
            "  [0.267 0.2   0.192]\n",
            "  [0.235 0.192 0.18 ]\n",
            "  [0.184 0.157 0.149]]\n",
            "\n",
            " [[0.161 0.141 0.118]\n",
            "  [0.075 0.059 0.063]\n",
            "  [0.082 0.039 0.067]\n",
            "  ...\n",
            "  [0.255 0.188 0.188]\n",
            "  [0.216 0.169 0.165]\n",
            "  [0.141 0.122 0.125]]] (120, 160, 3)\n"
          ]
        }
      ],
      "source": [
        "print(train_image_dict['15570'],train_image_dict['15570'].shape)"
      ],
      "id": "M4B9b6LkXCFA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "fO7us_UF_9Rt",
        "outputId": "0e894689-34b6-4e61-85d6-c1b8069a4b44"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntrain_image_dict = {}\\nval_image_dict = {}\\ntrain_image_dict = convert_all_images_to_numpy(new_train_map_df, \\'train\\')\\nval_image_dict = convert_all_images_to_numpy(new_val_map_df, \\'val\\')\\n\\ntrain_image_dict_file = ROOT_PATH + \"/\" + \"train_dict.npz\"\\nval_image_dict_file = ROOT_PATH + \"/\" + \"val_dict.npz\"\\n\\ntrain_image_dict_str = {str(key): value for key, value in train_image_dict.items()}\\nnp.savez_compressed(train_image_dict_file, **train_image_dict_str)\\nval_image_dict_str = {str(key): value for key, value in val_image_dict.items()}\\nnp.savez_compressed(val_image_dict_file, **val_image_dict_str)\\n\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "train_image_dict = {}\n",
        "val_image_dict = {}\n",
        "train_image_dict = convert_all_images_to_numpy(new_train_map_df, 'train')\n",
        "val_image_dict = convert_all_images_to_numpy(new_val_map_df, 'val')\n",
        "\n",
        "train_image_dict_file = ROOT_PATH + \"/\" + \"train_dict.npz\"\n",
        "val_image_dict_file = ROOT_PATH + \"/\" + \"val_dict.npz\"\n",
        "\n",
        "train_image_dict_str = {str(key): value for key, value in train_image_dict.items()}\n",
        "np.savez_compressed(train_image_dict_file, **train_image_dict_str)\n",
        "val_image_dict_str = {str(key): value for key, value in val_image_dict.items()}\n",
        "np.savez_compressed(val_image_dict_file, **val_image_dict_str)\n",
        "\n",
        "'''"
      ],
      "id": "fO7us_UF_9Rt"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "b7e60716"
      },
      "outputs": [],
      "source": [
        "train_size = len(train_map_df)\n",
        "val_size = len(val_map_df)\n",
        "\n",
        "if (train_size%batch_size) == 0:\n",
        "    steps_per_epoch = int(train_size/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (train_size//batch_size) + 1\n",
        "\n",
        "if (val_size%batch_size) == 0:\n",
        "    validation_steps = int(val_size/batch_size)\n",
        "else:\n",
        "    validation_steps = (val_size//batch_size) + 1"
      ],
      "id": "b7e60716"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e39e74df",
        "outputId": "81c4ec4e-c5a4-49dd-8141-d61755596c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "663 100 21 4\n"
          ]
        }
      ],
      "source": [
        "print(train_size,val_size,steps_per_epoch,validation_steps)"
      ],
      "id": "e39e74df"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "171ee62b"
      },
      "outputs": [],
      "source": [
        "#print(new_train_map_df.head(), new_val_map_df.tail)"
      ],
      "id": "171ee62b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg2YOyaZR8g2"
      },
      "source": [],
      "id": "gg2YOyaZR8g2"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "e2b6819b"
      },
      "outputs": [],
      "source": [
        "train_batch_generator =  batch_generator(new_train_map_df, batch_size, 'train', train_image_dict)\n",
        "val_batch_generator =  batch_generator(new_val_map_df, batch_size, 'val', val_image_dict)"
      ],
      "id": "e2b6819b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ede2795"
      },
      "outputs": [],
      "source": [
        "#shuffled_df = new_train_map_df.groupby('Folder_Name', group_keys=False).apply(lambda x: x.sample(frac=1))\n",
        "#print(shuffled_df, type(shuffled_df))"
      ],
      "id": "3ede2795"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ce02f2a"
      },
      "outputs": [],
      "source": [
        "# Let us try video classification using conv3D model\n",
        "\n",
        "input_size_tuple = (num_samples, img_width, img_height, num_of_channels)\n",
        "\n",
        "# input layer\n",
        "input_layer = Input(input_size_tuple)\n",
        "# Conv layers\n",
        "conv_layer1 = Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu')(input_layer)\n",
        "conv_layer2 = Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu')(conv_layer1)\n",
        "# max pooling layer\n",
        "pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer2)\n",
        "\n",
        "conv_layer3 = Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu')(pooling_layer1)\n",
        "conv_layer4 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(conv_layer3)\n",
        "pooling_layer2 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer4)\n",
        "\n",
        "#pooling_layer2 = BatchNormalization()(pooling_layer2)\n",
        "flatten_layer = Flatten()(pooling_layer2)\n",
        "\n",
        "## add dropouts to avoid overfitting / perform regularization\n",
        "dense_layer1 = Dense(units=128, activation='relu')(flatten_layer)\n",
        "dense_layer1 = Dropout(0.2)(dense_layer1)\n",
        "dense_layer2 = Dense(units=64, activation='relu')(dense_layer1)\n",
        "dense_layer2 = Dropout(0.2)(dense_layer2)\n",
        "output_layer = Dense(units=num_of_classes, activation='softmax')(dense_layer1)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n"
      ],
      "id": "7ce02f2a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a974c04",
        "outputId": "4ed54474-8e6c-43fe-f6cb-078f0a57e4fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 180, 180, 3)  0         \n",
            "                             ]                                   \n",
            "                                                                 \n",
            " conv3d (Conv3D)             (None, 26, 178, 178, 8)   656       \n",
            "                                                                 \n",
            " conv3d_1 (Conv3D)           (None, 24, 176, 176, 16)  3472      \n",
            "                                                                 \n",
            " max_pooling3d (MaxPooling3D  (None, 12, 88, 88, 16)   0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv3d_2 (Conv3D)           (None, 10, 86, 86, 32)    13856     \n",
            "                                                                 \n",
            " conv3d_3 (Conv3D)           (None, 8, 84, 84, 64)     55360     \n",
            "                                                                 \n",
            " max_pooling3d_1 (MaxPooling  (None, 4, 42, 42, 64)    0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 451584)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               57802880  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 57,876,869\n",
            "Trainable params: 57,876,869\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ],
      "id": "1a974c04"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cb88e1f"
      },
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.001)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['acc'])"
      ],
      "id": "7cb88e1f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrqzmUQwSaBD"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import History \n",
        "history = History()\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "# let us create checkpoint so that weights are saved after every epoch\n",
        "checkpoint_filepath = ROOT_PATH + \"/\" + \"ccn_checkpoint_1.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_filepath, save_weights_only=True)\n"
      ],
      "id": "FrqzmUQwSaBD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "c4d0cfa4",
        "outputId": "b4e3d7a1-34b4-411d-a29c-8e1e6f9afd38"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#with tensorflow.device('/CPU:0'):\\nmodel.fit(train_batch_generator, validation_data=val_batch_generator, \\n                steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=epochs, \\n                verbose=1, callbacks=[checkpoint, history, reduce_lr])\\n\""
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "#with tensorflow.device('/CPU:0'):\n",
        "model.fit(train_batch_generator, validation_data=val_batch_generator, \n",
        "                steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=epochs, \n",
        "                verbose=1, callbacks=[checkpoint, history, reduce_lr])\n",
        "'''"
      ],
      "id": "c4d0cfa4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "19b9e438",
        "outputId": "84600200-3421-4954-89ac-05693f3a0222"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nacc = history.history['acc']\\nval_acc = history.history['val_acc']\\n\\nloss = history.history['loss']\\nval_loss = history.history['val_loss']\\n\\nepochs_range = range(epochs)\\n\\nplt.figure(figsize=(8, 8))\\nplt.subplot(1, 2, 1)\\nplt.plot(epochs_range, acc, label='Training Accuracy')\\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\\nplt.legend(loc='lower right')\\nplt.title('Training and Validation Accuracy')\\n\\nplt.subplot(1, 2, 2)\\nplt.plot(epochs_range, loss, label='Training Loss')\\nplt.plot(epochs_range, val_loss, label='Validation Loss')\\nplt.legend(loc='upper right')\\nplt.title('Training and Validation Loss')\\nplt.show()\\n\""
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "'''"
      ],
      "id": "19b9e438"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jndIcRGdjby6"
      },
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.0005)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['acc'])"
      ],
      "id": "jndIcRGdjby6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjFLP-s7jft8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import History \n",
        "history = History()\n",
        "\n",
        "epochs = 40\n",
        "\n",
        "# let us create checkpoint so that weights are saved after every epoch\n",
        "checkpoint_filepath = ROOT_PATH + \"/\" + \"ccn_checkpoint_2.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_filepath, save_weights_only=True)"
      ],
      "id": "XjFLP-s7jft8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgwWDk7-u9dZ"
      },
      "outputs": [],
      "source": [],
      "id": "dgwWDk7-u9dZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O91PQICqjjhE",
        "outputId": "80d9c95e-5674-4856-e13d-5983500961d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "21/21 [==============================] - 128s 6s/step - loss: 1.5862 - acc: 0.2068 - val_loss: 1.2351 - val_acc: 0.2031 - lr: 5.0000e-04\n",
            "Epoch 2/40\n",
            "21/21 [==============================] - 120s 6s/step - loss: 1.5028 - acc: 0.2976 - val_loss: 1.1369 - val_acc: 0.3203 - lr: 5.0000e-04\n",
            "Epoch 3/40\n",
            "21/21 [==============================] - 120s 6s/step - loss: 1.3251 - acc: 0.4464 - val_loss: 0.9238 - val_acc: 0.3672 - lr: 5.0000e-04\n",
            "Epoch 4/40\n",
            "21/21 [==============================] - 117s 6s/step - loss: 1.0998 - acc: 0.5417 - val_loss: 0.8685 - val_acc: 0.3984 - lr: 5.0000e-04\n",
            "Epoch 5/40\n",
            "21/21 [==============================] - 119s 6s/step - loss: 0.9223 - acc: 0.6458 - val_loss: 0.9872 - val_acc: 0.3906 - lr: 5.0000e-04\n",
            "Epoch 6/40\n",
            "21/21 [==============================] - 119s 6s/step - loss: 0.7471 - acc: 0.7351 - val_loss: 1.0006 - val_acc: 0.4219 - lr: 5.0000e-04\n",
            "Epoch 7/40\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.5867 - acc: 0.7738 - val_loss: 0.9430 - val_acc: 0.4062 - lr: 5.0000e-04\n",
            "Epoch 8/40\n",
            "21/21 [==============================] - 117s 6s/step - loss: 0.4037 - acc: 0.8482 - val_loss: 1.1944 - val_acc: 0.3672 - lr: 5.0000e-04\n",
            "Epoch 9/40\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.3523 - acc: 0.8720 - val_loss: 1.6435 - val_acc: 0.2891 - lr: 5.0000e-04\n",
            "Epoch 10/40\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.4676 - acc: 0.8318 - val_loss: 1.1535 - val_acc: 0.3594 - lr: 5.0000e-04\n",
            "Epoch 11/40\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.3810 - acc: 0.8482 - val_loss: 1.2487 - val_acc: 0.3594 - lr: 5.0000e-04\n",
            "Epoch 12/40\n",
            "21/21 [==============================] - 117s 6s/step - loss: 0.1964 - acc: 0.9182 - val_loss: 1.7412 - val_acc: 0.3359 - lr: 5.0000e-04\n",
            "Epoch 13/40\n",
            "21/21 [==============================] - 119s 6s/step - loss: 0.1264 - acc: 0.9598 - val_loss: 1.6509 - val_acc: 0.3281 - lr: 5.0000e-04\n",
            "Epoch 14/40\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.1216 - acc: 0.9420 - val_loss: 1.8313 - val_acc: 0.3359 - lr: 5.0000e-04\n",
            "Epoch 15/40\n",
            "21/21 [==============================] - 119s 6s/step - loss: 0.1035 - acc: 0.9509 - val_loss: 2.0370 - val_acc: 0.3203 - lr: 5.0000e-04\n",
            "Epoch 16/40\n",
            "21/21 [==============================] - 117s 6s/step - loss: 0.1152 - acc: 0.9524 - val_loss: 1.6908 - val_acc: 0.3516 - lr: 5.0000e-04\n",
            "Epoch 17/40\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.1700 - acc: 0.9345 - val_loss: 1.8292 - val_acc: 0.2891 - lr: 5.0000e-04\n",
            "Epoch 18/40\n",
            "21/21 [==============================] - 121s 6s/step - loss: 0.1513 - acc: 0.9375 - val_loss: 2.0160 - val_acc: 0.3203 - lr: 5.0000e-04\n",
            "Epoch 19/40\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.0852 - acc: 0.9658 - val_loss: 1.9352 - val_acc: 0.3359 - lr: 5.0000e-04\n",
            "Epoch 20/40\n",
            "21/21 [==============================] - 117s 6s/step - loss: 0.0767 - acc: 0.9673 - val_loss: 2.1032 - val_acc: 0.3359 - lr: 5.0000e-04\n",
            "Epoch 21/40\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.0607 - acc: 0.9732 - val_loss: 1.9419 - val_acc: 0.3203 - lr: 5.0000e-04\n",
            "Epoch 22/40\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.0366 - acc: 0.9747 - val_loss: 2.3891 - val_acc: 0.3203 - lr: 5.0000e-04\n",
            "Epoch 23/40\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.0158 - acc: 0.9851 - val_loss: 2.4704 - val_acc: 0.2891 - lr: 5.0000e-04\n",
            "Epoch 24/40\n",
            "21/21 [==============================] - 117s 6s/step - loss: 0.0087 - acc: 0.9866 - val_loss: 2.6844 - val_acc: 0.2734 - lr: 5.0000e-04\n",
            "Epoch 25/40\n",
            "20/21 [===========================>..] - ETA: 5s - loss: 0.0093 - acc: 0.9984 "
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-7696547b3989>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#with tensorflow.device('/CPU:0'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model.fit(train_batch_generator, validation_data=val_batch_generator, \n\u001b[0m\u001b[1;32m      3\u001b[0m                 \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 verbose=1, callbacks=[checkpoint, history, reduce_lr])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#with tensorflow.device('/CPU:0'):\n",
        "model.fit(train_batch_generator, validation_data=val_batch_generator, \n",
        "                steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=epochs, \n",
        "                verbose=1, callbacks=[checkpoint, history, reduce_lr])"
      ],
      "id": "O91PQICqjjhE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95i5oj79uvt5"
      },
      "outputs": [],
      "source": [
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "id": "95i5oj79uvt5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ak004gBun_J"
      },
      "outputs": [],
      "source": [
        "# Let us try video classification using conv3D model - simplified model\n",
        "\n",
        "input_size_tuple = (num_samples, img_width, img_height, num_of_channels)\n",
        "\n",
        "# input layer\n",
        "input_layer = Input(input_size_tuple)\n",
        "# Conv layers\n",
        "conv_layer1 = Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu')(input_layer)\n",
        "conv_layer2 = Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu')(conv_layer1)\n",
        "# max pooling layer\n",
        "pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer2)\n",
        "\n",
        "flatten_layer = Flatten()(pooling_layer1)\n",
        "\n",
        "## add dropouts to avoid overfitting / perform regularization\n",
        "dense_layer2 = Dense(units=64, activation='relu')(flatten_layer)\n",
        "dense_layer2 = Dropout(0.2)(dense_layer2)\n",
        "output_layer = Dense(units=num_of_classes, activation='softmax')(dense_layer2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)"
      ],
      "id": "8ak004gBun_J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij9nlS5Yu7A4"
      },
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.0005)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['acc'])"
      ],
      "id": "ij9nlS5Yu7A4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAlJM-livATJ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import History \n",
        "history = History()\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "# let us create checkpoint so that weights are saved after every epoch\n",
        "checkpoint_filepath = ROOT_PATH + \"/\" + \"ccn_checkpoint_3.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_filepath, save_weights_only=True)"
      ],
      "id": "YAlJM-livATJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLf-PZ1Sv6ix",
        "outputId": "a58dfaf0-7550-4615-d714-175c7e30e8f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "21/21 [==============================] - 120s 6s/step - loss: 1.5988 - acc: 0.2202 - val_loss: 1.1992 - val_acc: 0.2188 - lr: 5.0000e-04\n",
            "Epoch 2/30\n",
            "21/21 [==============================] - 120s 6s/step - loss: 1.4499 - acc: 0.3244 - val_loss: 1.1266 - val_acc: 0.2812 - lr: 5.0000e-04\n",
            "Epoch 3/30\n",
            "21/21 [==============================] - 120s 6s/step - loss: 1.2946 - acc: 0.4732 - val_loss: 1.0103 - val_acc: 0.3594 - lr: 5.0000e-04\n",
            "Epoch 4/30\n",
            "21/21 [==============================] - 117s 6s/step - loss: 1.0801 - acc: 0.5685 - val_loss: 0.9032 - val_acc: 0.4453 - lr: 5.0000e-04\n",
            "Epoch 5/30\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.8117 - acc: 0.7098 - val_loss: 0.8730 - val_acc: 0.4531 - lr: 5.0000e-04\n",
            "Epoch 6/30\n",
            "21/21 [==============================] - 121s 6s/step - loss: 0.6892 - acc: 0.7753 - val_loss: 1.2269 - val_acc: 0.3438 - lr: 5.0000e-04\n",
            "Epoch 7/30\n",
            "21/21 [==============================] - 121s 6s/step - loss: 0.5988 - acc: 0.7902 - val_loss: 0.9018 - val_acc: 0.4141 - lr: 5.0000e-04\n",
            "Epoch 8/30\n",
            "21/21 [==============================] - 118s 6s/step - loss: 0.4485 - acc: 0.8586 - val_loss: 1.0172 - val_acc: 0.3828 - lr: 5.0000e-04\n",
            "Epoch 9/30\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.3843 - acc: 0.8646 - val_loss: 1.0837 - val_acc: 0.3359 - lr: 5.0000e-04\n",
            "Epoch 10/30\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.3485 - acc: 0.8973 - val_loss: 1.1096 - val_acc: 0.3438 - lr: 5.0000e-04\n",
            "Epoch 11/30\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.3196 - acc: 0.9033 - val_loss: 1.2983 - val_acc: 0.3828 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "21/21 [==============================] - 118s 6s/step - loss: 0.2868 - acc: 0.9048 - val_loss: 1.1212 - val_acc: 0.3594 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "21/21 [==============================] - 121s 6s/step - loss: 0.1843 - acc: 0.9449 - val_loss: 1.2778 - val_acc: 0.3438 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "21/21 [==============================] - 121s 6s/step - loss: 0.1602 - acc: 0.9509 - val_loss: 1.1483 - val_acc: 0.3828 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "21/21 [==============================] - 122s 6s/step - loss: 0.1803 - acc: 0.9509 - val_loss: 1.1976 - val_acc: 0.3281 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "21/21 [==============================] - 117s 6s/step - loss: 0.1565 - acc: 0.9539 - val_loss: 1.1372 - val_acc: 0.3438 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "21/21 [==============================] - 121s 6s/step - loss: 0.1587 - acc: 0.9568 - val_loss: 1.0950 - val_acc: 0.3906 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "21/21 [==============================] - 121s 6s/step - loss: 0.1190 - acc: 0.9494 - val_loss: 1.0777 - val_acc: 0.4297 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.1214 - acc: 0.9539 - val_loss: 1.2999 - val_acc: 0.4141 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "21/21 [==============================] - 117s 6s/step - loss: 0.1006 - acc: 0.9643 - val_loss: 1.2347 - val_acc: 0.3828 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.0819 - acc: 0.9673 - val_loss: 1.4247 - val_acc: 0.3984 - lr: 5.0000e-04\n",
            "Epoch 22/30\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.0737 - acc: 0.9643 - val_loss: 1.2520 - val_acc: 0.4297 - lr: 5.0000e-04\n",
            "Epoch 23/30\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.0654 - acc: 0.9643 - val_loss: 1.2398 - val_acc: 0.4219 - lr: 5.0000e-04\n",
            "Epoch 24/30\n",
            "21/21 [==============================] - 117s 6s/step - loss: 0.0605 - acc: 0.9702 - val_loss: 1.4182 - val_acc: 0.4219 - lr: 5.0000e-04\n",
            "Epoch 25/30\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.0535 - acc: 0.9762 - val_loss: 1.4726 - val_acc: 0.3828 - lr: 5.0000e-04\n",
            "Epoch 26/30\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.0538 - acc: 0.9732 - val_loss: 1.3988 - val_acc: 0.4062 - lr: 5.0000e-04\n",
            "Epoch 27/30\n",
            "21/21 [==============================] - 121s 6s/step - loss: 0.0856 - acc: 0.9598 - val_loss: 1.5604 - val_acc: 0.4219 - lr: 5.0000e-04\n",
            "Epoch 28/30\n",
            "21/21 [==============================] - 117s 6s/step - loss: 0.0758 - acc: 0.9702 - val_loss: 1.4848 - val_acc: 0.3828 - lr: 5.0000e-04\n",
            "Epoch 29/30\n",
            "21/21 [==============================] - 120s 6s/step - loss: 0.0593 - acc: 0.9777 - val_loss: 1.3323 - val_acc: 0.3906 - lr: 5.0000e-04\n",
            "Epoch 30/30\n",
            "21/21 [==============================] - 121s 6s/step - loss: 0.0368 - acc: 0.9851 - val_loss: 1.4507 - val_acc: 0.4062 - lr: 5.0000e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc29c51d450>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#with tensorflow.device('/CPU:0'):\n",
        "model.fit(train_batch_generator, validation_data=val_batch_generator, \n",
        "                steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=epochs, \n",
        "                verbose=1, callbacks=[checkpoint, history, reduce_lr])"
      ],
      "id": "yLf-PZ1Sv6ix"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPo-dpaME8Gc"
      },
      "outputs": [],
      "source": [
        "# Let us try video classification using conv3D model \n",
        "# Image size is 120 X 160\n",
        "\n",
        "img_width, img_height = 120, 160\n",
        "input_size_tuple = (num_samples, img_width, img_height, num_of_channels)\n",
        "\n",
        "# input layer\n",
        "input_layer = Input(input_size_tuple)\n",
        "# Conv layers\n",
        "conv_layer1 = Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu')(input_layer)\n",
        "conv_layer2 = Conv3D(filters=16, kernel_size=(3, 3, 3), activation='relu')(conv_layer1)\n",
        "# max pooling layer\n",
        "pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer2)\n",
        "\n",
        "conv_layer3 = Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu')(pooling_layer1)\n",
        "conv_layer4 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(conv_layer3)\n",
        "pooling_layer2 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer4)\n",
        "\n",
        "#pooling_layer2 = BatchNormalization()(pooling_layer2)\n",
        "flatten_layer = Flatten()(pooling_layer2)\n",
        "\n",
        "## add dropouts to avoid overfitting / perform regularization\n",
        "dense_layer1 = Dense(units=128, activation='relu')(flatten_layer)\n",
        "dense_layer1 = Dropout(0.2)(dense_layer1)\n",
        "dense_layer2 = Dense(units=64, activation='relu')(dense_layer1)\n",
        "dense_layer2 = Dropout(0.2)(dense_layer2)\n",
        "output_layer = Dense(units=num_of_classes, activation='softmax')(dense_layer2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)"
      ],
      "id": "PPo-dpaME8Gc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn4dzJG1FFBv"
      },
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.0005)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['acc'])"
      ],
      "id": "Pn4dzJG1FFBv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F64MYLq5FLEu"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import History \n",
        "history = History()\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "# let us create checkpoint so that weights are saved after every epoch\n",
        "checkpoint_filepath = ROOT_PATH + \"/\" + \"ccn_checkpoint_4.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_filepath, save_weights_only=True)"
      ],
      "id": "F64MYLq5FLEu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsSTDe0UlQM7"
      },
      "outputs": [],
      "source": [],
      "id": "tsSTDe0UlQM7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND0wSE9NFRxZ",
        "outputId": "3a2354dd-8e57-48c2-eae7-d4b057f3d411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "21/21 [==============================] - 103s 5s/step - loss: 1.5867 - acc: 0.2232 - val_loss: 1.2433 - val_acc: 0.2031 - lr: 5.0000e-04\n",
            "Epoch 2/30\n",
            "21/21 [==============================] - 93s 5s/step - loss: 1.5245 - acc: 0.2768 - val_loss: 1.1488 - val_acc: 0.3281 - lr: 5.0000e-04\n",
            "Epoch 3/30\n",
            "21/21 [==============================] - 93s 5s/step - loss: 1.3517 - acc: 0.4390 - val_loss: 0.9854 - val_acc: 0.3750 - lr: 5.0000e-04\n",
            "Epoch 4/30\n",
            "21/21 [==============================] - 90s 5s/step - loss: 1.1740 - acc: 0.4940 - val_loss: 0.8864 - val_acc: 0.4219 - lr: 5.0000e-04\n",
            "Epoch 5/30\n",
            "21/21 [==============================] - 93s 5s/step - loss: 1.0169 - acc: 0.6012 - val_loss: 0.8697 - val_acc: 0.4375 - lr: 5.0000e-04\n",
            "Epoch 6/30\n",
            "21/21 [==============================] - 93s 5s/step - loss: 0.8942 - acc: 0.6562 - val_loss: 0.8304 - val_acc: 0.4453 - lr: 5.0000e-04\n",
            "Epoch 7/30\n",
            "21/21 [==============================] - 93s 5s/step - loss: 0.7712 - acc: 0.6949 - val_loss: 0.8050 - val_acc: 0.4219 - lr: 5.0000e-04\n",
            "Epoch 8/30\n",
            "21/21 [==============================] - 91s 5s/step - loss: 0.7008 - acc: 0.7307 - val_loss: 0.9268 - val_acc: 0.4062 - lr: 5.0000e-04\n",
            "Epoch 9/30\n",
            "21/21 [==============================] - 93s 5s/step - loss: 0.6758 - acc: 0.7336 - val_loss: 0.8583 - val_acc: 0.3984 - lr: 5.0000e-04\n",
            "Epoch 10/30\n",
            "21/21 [==============================] - 93s 5s/step - loss: 0.5107 - acc: 0.8110 - val_loss: 1.1728 - val_acc: 0.3750 - lr: 5.0000e-04\n",
            "Epoch 11/30\n",
            "21/21 [==============================] - 93s 5s/step - loss: 0.3262 - acc: 0.8690 - val_loss: 1.4784 - val_acc: 0.3516 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "21/21 [==============================] - 91s 5s/step - loss: 0.2907 - acc: 0.8869 - val_loss: 1.3815 - val_acc: 0.3828 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "21/21 [==============================] - 94s 5s/step - loss: 0.3298 - acc: 0.8676 - val_loss: 1.5260 - val_acc: 0.3594 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "21/21 [==============================] - 93s 5s/step - loss: 0.2473 - acc: 0.8914 - val_loss: 1.6644 - val_acc: 0.3516 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "21/21 [==============================] - 93s 5s/step - loss: 0.2887 - acc: 0.8839 - val_loss: 1.2655 - val_acc: 0.3516 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "21/21 [==============================] - 91s 5s/step - loss: 0.3598 - acc: 0.8527 - val_loss: 1.3705 - val_acc: 0.3750 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "21/21 [==============================] - 93s 5s/step - loss: 0.2862 - acc: 0.8824 - val_loss: 1.4835 - val_acc: 0.3750 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "21/21 [==============================] - 93s 5s/step - loss: 0.2363 - acc: 0.9137 - val_loss: 1.6379 - val_acc: 0.3125 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "21/21 [==============================] - 94s 5s/step - loss: 0.3709 - acc: 0.8467 - val_loss: 1.5431 - val_acc: 0.3281 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "21/21 [==============================] - 91s 5s/step - loss: 0.2836 - acc: 0.8929 - val_loss: 1.9188 - val_acc: 0.3906 - lr: 5.0000e-04\n",
            "Epoch 21/30\n",
            "21/21 [==============================] - 93s 5s/step - loss: 0.1137 - acc: 0.9568 - val_loss: 2.1851 - val_acc: 0.3672 - lr: 5.0000e-04\n",
            "Epoch 22/30\n",
            "21/21 [==============================] - 93s 5s/step - loss: 0.1121 - acc: 0.9524 - val_loss: 1.9457 - val_acc: 0.4062 - lr: 5.0000e-04\n",
            "Epoch 23/30\n",
            "21/21 [==============================] - 93s 5s/step - loss: 0.1097 - acc: 0.9539 - val_loss: 1.8879 - val_acc: 0.4141 - lr: 5.0000e-04\n",
            "Epoch 24/30\n",
            "21/21 [==============================] - 92s 5s/step - loss: 0.0777 - acc: 0.9673 - val_loss: 1.9865 - val_acc: 0.3750 - lr: 5.0000e-04\n",
            "Epoch 25/30\n",
            "21/21 [==============================] - 94s 5s/step - loss: 0.0526 - acc: 0.9702 - val_loss: 2.1767 - val_acc: 0.3984 - lr: 5.0000e-04\n",
            "Epoch 26/30\n",
            "21/21 [==============================] - 94s 5s/step - loss: 0.0283 - acc: 0.9821 - val_loss: 2.1888 - val_acc: 0.4531 - lr: 5.0000e-04\n",
            "Epoch 27/30\n",
            "21/21 [==============================] - 96s 5s/step - loss: 0.0117 - acc: 0.9866 - val_loss: 2.3714 - val_acc: 0.4062 - lr: 5.0000e-04\n",
            "Epoch 28/30\n",
            "21/21 [==============================] - 92s 5s/step - loss: 0.0097 - acc: 0.9836 - val_loss: 2.3909 - val_acc: 0.4219 - lr: 5.0000e-04\n",
            "Epoch 29/30\n",
            "21/21 [==============================] - 95s 5s/step - loss: 0.0086 - acc: 0.9866 - val_loss: 2.4958 - val_acc: 0.4062 - lr: 5.0000e-04\n",
            "Epoch 30/30\n",
            "21/21 [==============================] - 94s 5s/step - loss: 0.0116 - acc: 0.9866 - val_loss: 2.6069 - val_acc: 0.4141 - lr: 5.0000e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc29c5c99f0>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#with tensorflow.device('/CPU:0'):\n",
        "model.fit(train_batch_generator, validation_data=val_batch_generator, \n",
        "                steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=epochs, \n",
        "                verbose=1, callbacks=[checkpoint, history, reduce_lr])"
      ],
      "id": "ND0wSE9NFRxZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMWPLaUsgORE"
      },
      "outputs": [],
      "source": [
        "\n",
        "batch_size = 32\n",
        "# we have 30 images, but we chose 24 below and we pick them randomly and sort to ensure temporal dependence\n",
        "num_samples = 28\n",
        "num_of_channels = 3\n",
        "num_of_classes = 5\n",
        "\n",
        "img_width, img_height = 120, 160\n",
        "\n",
        "# Let us try video classification using conv3D model - another method\n",
        "\n",
        "input_size_tuple = (num_samples, img_width, img_height, num_of_channels)\n",
        "\n",
        "# input layer\n",
        "input_layer = Input(input_size_tuple)\n",
        "# Conv layers\n",
        "conv_layer1 = Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu')(input_layer)\n",
        "#batchnorm_layer1 = BatchNormalization()(conv_layer1)\n",
        "pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer1)\n",
        "\n",
        "conv_layer2 = Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')(pooling_layer1)\n",
        "#batchnorm_layer2 = BatchNormalization()(conv_layer2)\n",
        "pooling_layer2 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer2)\n",
        "\n",
        "conv_layer3 = Conv3D(filters=128, kernel_size=(3, 3, 3), activation='relu')(pooling_layer2)\n",
        "#batchnorm_layer3 = BatchNormalization()(conv_layer3)\n",
        "pooling_layer3 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer3)\n",
        "\n",
        "#pooling_layer2 = BatchNormalization()(pooling_layer2)\n",
        "flatten_layer = Flatten()(pooling_layer3)\n",
        "\n",
        "## add dropouts to avoid overfitting / perform regularization\n",
        "dense_layer1 = Dense(units=512, activation='relu')(flatten_layer)\n",
        "dense_layer1 = Dropout(0.3)(dense_layer1)\n",
        "dense_layer2 = Dense(units=256, activation='relu')(dense_layer1)\n",
        "dense_layer2 = Dropout(0.3)(dense_layer2)\n",
        "output_layer = Dense(units=num_of_classes, activation='softmax')(dense_layer2)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n"
      ],
      "id": "KMWPLaUsgORE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G7inU4ZlNtj"
      },
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.0005)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['acc'])"
      ],
      "id": "8G7inU4ZlNtj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IfuZB5jlRJ7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import History \n",
        "history = History()\n",
        "\n",
        "epochs = 15\n",
        "\n",
        "# let us create checkpoint so that weights are saved after every epoch\n",
        "checkpoint_filepath = ROOT_PATH + \"/\" + \"ccn_checkpoint_5.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_filepath, save_weights_only=True)"
      ],
      "id": "4IfuZB5jlRJ7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1Qd0aVilfEF",
        "outputId": "7db9c378-a619-4a72-dd65-4fb5d4f21d86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 28, 120, 160, 3)  0         \n",
            "                             ]                                   \n",
            "                                                                 \n",
            " conv3d_9 (Conv3D)           (None, 26, 118, 158, 32)  2624      \n",
            "                                                                 \n",
            " max_pooling3d_9 (MaxPooling  (None, 13, 59, 79, 32)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_10 (Conv3D)          (None, 11, 57, 77, 64)    55360     \n",
            "                                                                 \n",
            " max_pooling3d_10 (MaxPoolin  (None, 5, 28, 38, 64)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_11 (Conv3D)          (None, 3, 26, 36, 128)    221312    \n",
            "                                                                 \n",
            " max_pooling3d_11 (MaxPoolin  (None, 1, 13, 18, 128)   0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 29952)             0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 512)               15335936  \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,747,845\n",
            "Trainable params: 15,747,845\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ],
      "id": "W1Qd0aVilfEF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3T-ysvLlXdT",
        "outputId": "7a407897-d493-4e12-927e-1c0bbef15998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "21/21 [==============================] - 47s 2s/step - loss: 1.5794 - acc: 0.1815 - val_loss: 1.3069 - val_acc: 0.1719 - lr: 5.0000e-04\n",
            "Epoch 2/15\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.6071 - acc: 0.1994 - val_loss: 1.2778 - val_acc: 0.1875 - lr: 5.0000e-04\n",
            "Epoch 3/15\n",
            "21/21 [==============================] - 47s 2s/step - loss: 1.5523 - acc: 0.2113 - val_loss: 1.5725 - val_acc: 0.2656 - lr: 5.0000e-04\n",
            "Epoch 4/15\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.5400 - acc: 0.2589 - val_loss: 1.2099 - val_acc: 0.2969 - lr: 5.0000e-04\n",
            "Epoch 5/15\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.4086 - acc: 0.3661 - val_loss: 1.0733 - val_acc: 0.3906 - lr: 5.0000e-04\n",
            "Epoch 6/15\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.4011 - acc: 0.3363 - val_loss: 1.3670 - val_acc: 0.3438 - lr: 5.0000e-04\n",
            "Epoch 7/15\n",
            "21/21 [==============================] - 47s 2s/step - loss: 1.2716 - acc: 0.3780 - val_loss: 1.2828 - val_acc: 0.3281 - lr: 5.0000e-04\n",
            "Epoch 8/15\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.2847 - acc: 0.3482 - val_loss: 1.3675 - val_acc: 0.1719 - lr: 5.0000e-04\n",
            "Epoch 9/15\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.2256 - acc: 0.4315 - val_loss: 0.9622 - val_acc: 0.3906 - lr: 5.0000e-04\n",
            "Epoch 10/15\n",
            "21/21 [==============================] - 47s 2s/step - loss: 1.2152 - acc: 0.4048 - val_loss: 1.1799 - val_acc: 0.5000 - lr: 5.0000e-04\n",
            "Epoch 11/15\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.1808 - acc: 0.4226 - val_loss: 1.0553 - val_acc: 0.2969 - lr: 5.0000e-04\n",
            "Epoch 12/15\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.1694 - acc: 0.4345 - val_loss: 0.9803 - val_acc: 0.3125 - lr: 5.0000e-04\n",
            "Epoch 13/15\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.1170 - acc: 0.4643 - val_loss: 1.2610 - val_acc: 0.5000 - lr: 5.0000e-04\n",
            "Epoch 14/15\n",
            "21/21 [==============================] - 47s 2s/step - loss: 1.0942 - acc: 0.5000 - val_loss: 1.1576 - val_acc: 0.5625 - lr: 5.0000e-04\n",
            "Epoch 15/15\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.0364 - acc: 0.5060 - val_loss: 0.9617 - val_acc: 0.3750 - lr: 5.0000e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0acb35e1a0>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#with tensorflow.device('/CPU:0'):\n",
        "model.fit(train_batch_generator, validation_data=val_batch_generator, \n",
        "                steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=epochs, \n",
        "                verbose=1, callbacks=[checkpoint, history, reduce_lr])"
      ],
      "id": "j3T-ysvLlXdT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOESAzC6_GCo",
        "outputId": "c67dbfda-9c5e-4af9-d3c0-b3098118438a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 340480)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# we have 30 images, but we chose 24 below and we pick them randomly and sort to ensure temporal dependence\n",
        "num_samples = 28\n",
        "num_of_channels = 3\n",
        "num_of_classes = 5\n",
        "batch_size = 32\n",
        "\n",
        "img_width, img_height = 120, 160\n",
        "\n",
        "# Let us try video classification using conv3D model - another method\n",
        "\n",
        "input_size_tuple = (num_samples, img_width, img_height, num_of_channels)\n",
        "\n",
        "# input layer\n",
        "input_layer = Input(input_size_tuple)\n",
        "# Conv layers\n",
        "conv_layer1 = Conv3D(filters=8, kernel_size=(2, 2, 2), activation='relu')(input_layer)\n",
        "conv_layer2 = Conv3D(filters=16, kernel_size=(2, 2, 2), activation='relu')(conv_layer1)\n",
        "# max pooling layer\n",
        "pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer2)\n",
        "\n",
        "conv_layer3 = Conv3D(filters=32, kernel_size=(2, 2, 2), activation='relu')(pooling_layer1)\n",
        "conv_layer4 = Conv3D(filters=64, kernel_size=(2, 2, 2), activation='relu')(conv_layer3)\n",
        "pooling_layer2 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer4)\n",
        "\n",
        "#pooling_layer2 = BatchNormalization()(pooling_layer2)\n",
        "\n",
        "flatten_layer = Flatten()(pooling_layer2)\n",
        "print(flatten_layer.shape)\n",
        "\n",
        "timesteps = 28\n",
        "input_dim = flatten_layer.shape[1] // timesteps\n",
        "\n",
        "# Reshape to (batch_size, timesteps, input_dim)\n",
        "\n",
        "\n",
        "flatten_layer = Reshape((timesteps, input_dim) )(flatten_layer)\n",
        "\n",
        "lstm_layer1 = LSTM(64, return_sequences=True)(flatten_layer)\n",
        "lstm_layer2 = LSTM(16, return_sequences=False)(lstm_layer1)\n",
        "\n",
        "\n",
        "## add dropouts to avoid overfitting / perform regularization\n",
        "dense_layer1 = Dense(units=128, activation='relu')(lstm_layer2)\n",
        "dense_layer1 = Dropout(0.4)(dense_layer1)\n",
        "#dense_layer2 = Dense(units=64, activation='relu')(dense_layer1)\n",
        "#dense_layer2 = Dropout(0.4)(dense_layer2)\n",
        "\n",
        "output_layer = Dense(units=num_of_classes, activation='softmax')(dense_layer1)\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n"
      ],
      "id": "sOESAzC6_GCo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ9mBhEy_N5B"
      },
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.0005)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['acc'])"
      ],
      "id": "nQ9mBhEy_N5B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTB13TnP_Q8S"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import History \n",
        "history = History()\n",
        "\n",
        "epochs = 15\n",
        "\n",
        "# let us create checkpoint so that weights are saved after every epoch\n",
        "checkpoint_filepath = ROOT_PATH + \"/\" + \"ccn_checkpoint_5.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_filepath, save_weights_only=True)"
      ],
      "id": "DTB13TnP_Q8S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-ovm0NC_T1r",
        "outputId": "4e01bfa2-da6b-4a41-9a76-ef54b5957c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 28, 120, 160, 3)  0         \n",
            "                             ]                                   \n",
            "                                                                 \n",
            " conv3d_9 (Conv3D)           (None, 26, 118, 158, 32)  2624      \n",
            "                                                                 \n",
            " max_pooling3d_9 (MaxPooling  (None, 13, 59, 79, 32)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_10 (Conv3D)          (None, 11, 57, 77, 64)    55360     \n",
            "                                                                 \n",
            " max_pooling3d_10 (MaxPoolin  (None, 5, 28, 38, 64)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_11 (Conv3D)          (None, 3, 26, 36, 128)    221312    \n",
            "                                                                 \n",
            " max_pooling3d_11 (MaxPoolin  (None, 1, 13, 18, 128)   0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 29952)             0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 512)               15335936  \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,747,845\n",
            "Trainable params: 15,747,845\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ],
      "id": "n-ovm0NC_T1r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viOJFo4k_VhL",
        "outputId": "3a2c4a28-7cd2-47f0-edf9-9834f29f3c33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "21/21 [==============================] - 93s 5s/step - loss: 0.9778 - acc: 0.5938 - val_loss: 0.9690 - val_acc: 0.4297 - lr: 5.0000e-04\n",
            "Epoch 2/15\n",
            "21/21 [==============================] - 90s 4s/step - loss: 0.8804 - acc: 0.6071 - val_loss: 1.0993 - val_acc: 0.3594 - lr: 5.0000e-04\n",
            "Epoch 3/15\n",
            "21/21 [==============================] - 90s 4s/step - loss: 0.8446 - acc: 0.6473 - val_loss: 1.0870 - val_acc: 0.4297 - lr: 5.0000e-04\n",
            "Epoch 4/15\n",
            "21/21 [==============================] - 88s 4s/step - loss: 0.7554 - acc: 0.6815 - val_loss: 1.2429 - val_acc: 0.4062 - lr: 5.0000e-04\n",
            "Epoch 5/15\n",
            "21/21 [==============================] - 90s 5s/step - loss: 0.7113 - acc: 0.7143 - val_loss: 1.2885 - val_acc: 0.3828 - lr: 5.0000e-04\n",
            "Epoch 6/15\n",
            "21/21 [==============================] - 90s 5s/step - loss: 0.6507 - acc: 0.7277 - val_loss: 1.4658 - val_acc: 0.3828 - lr: 5.0000e-04\n",
            "Epoch 7/15\n",
            "21/21 [==============================] - 90s 5s/step - loss: 0.5875 - acc: 0.7470 - val_loss: 1.2710 - val_acc: 0.4062 - lr: 5.0000e-04\n",
            "Epoch 8/15\n",
            "21/21 [==============================] - 88s 4s/step - loss: 0.5815 - acc: 0.7634 - val_loss: 1.5889 - val_acc: 0.3281 - lr: 5.0000e-04\n",
            "Epoch 9/15\n",
            "21/21 [==============================] - 90s 5s/step - loss: 0.5759 - acc: 0.7679 - val_loss: 1.6961 - val_acc: 0.3438 - lr: 5.0000e-04\n",
            "Epoch 10/15\n",
            "21/21 [==============================] - 91s 5s/step - loss: 0.6243 - acc: 0.7500 - val_loss: 2.0966 - val_acc: 0.2734 - lr: 5.0000e-04\n",
            "Epoch 11/15\n",
            "21/21 [==============================] - 91s 5s/step - loss: 0.6289 - acc: 0.7426 - val_loss: 1.8107 - val_acc: 0.2812 - lr: 5.0000e-04\n",
            "Epoch 12/15\n",
            "21/21 [==============================] - 88s 4s/step - loss: 0.6336 - acc: 0.7500 - val_loss: 1.9788 - val_acc: 0.3359 - lr: 5.0000e-04\n",
            "Epoch 13/15\n",
            "21/21 [==============================] - 91s 5s/step - loss: 0.6157 - acc: 0.7798 - val_loss: 2.0299 - val_acc: 0.5469 - lr: 5.0000e-04\n",
            "Epoch 14/15\n",
            "21/21 [==============================] - 90s 5s/step - loss: 0.7033 - acc: 0.7173 - val_loss: 1.8260 - val_acc: 0.5312 - lr: 5.0000e-04\n",
            "Epoch 15/15\n",
            "21/21 [==============================] - 91s 5s/step - loss: 0.6157 - acc: 0.7515 - val_loss: 1.8691 - val_acc: 0.3672 - lr: 5.0000e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a28351a20>"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#with tensorflow.device('/CPU:0'):\n",
        "model.fit(train_batch_generator, validation_data=val_batch_generator, \n",
        "                steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=epochs, \n",
        "                verbose=1, callbacks=[checkpoint, history, reduce_lr])"
      ],
      "id": "viOJFo4k_VhL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECz14lloCVDb",
        "outputId": "53596505-74e3-46f5-8990-ca4d58a9e8d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 340480)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# we have 30 images, but we chose 24 below and we pick them randomly and sort to ensure temporal dependence\n",
        "num_samples = 28\n",
        "num_of_channels = 3\n",
        "num_of_classes = 5\n",
        "batch_size =16\n",
        "\n",
        "img_width, img_height = 120, 160\n",
        "\n",
        "# Let us try video classification using conv3D model - another method\n",
        "\n",
        "input_size_tuple = (num_samples, img_width, img_height, num_of_channels)\n",
        "\n",
        "# input layer\n",
        "input_layer = Input(input_size_tuple)\n",
        "# Conv layers\n",
        "conv_layer1 = Conv3D(filters=8, kernel_size=(2, 2, 2), activation='relu')(input_layer)\n",
        "conv_layer2 = Conv3D(filters=16, kernel_size=(2, 2, 2), activation='relu')(conv_layer1)\n",
        "# max pooling layer\n",
        "pooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer2)\n",
        "\n",
        "conv_layer3 = Conv3D(filters=32, kernel_size=(2, 2, 2), activation='relu')(pooling_layer1)\n",
        "conv_layer4 = Conv3D(filters=64, kernel_size=(2, 2, 2), activation='relu')(conv_layer3)\n",
        "pooling_layer2 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer4)\n",
        "\n",
        "#pooling_layer2 = BatchNormalization()(pooling_layer2)\n",
        "\n",
        "flatten_layer = Flatten()(pooling_layer2)\n",
        "print(flatten_layer.shape)\n",
        "\n",
        "timesteps = 5\n",
        "input_dim = flatten_layer.shape[1] // timesteps\n",
        "\n",
        "# Reshape to (batch_size, timesteps, input_dim)\n",
        "\n",
        "\n",
        "flatten_layer = Reshape((timesteps, input_dim) )(flatten_layer)\n",
        "\n",
        "lstm_layer1 = LSTM(128, return_sequences=True)(flatten_layer)\n",
        "lstm_layer2 = LSTM(32, return_sequences=False)(lstm_layer1)\n",
        "\n",
        "\n",
        "## add dropouts to avoid overfitting / perform regularization\n",
        "dense_layer1 = Dense(units=128, activation='relu')(lstm_layer2)\n",
        "dense_layer1 = Dropout(0.4)(dense_layer1)\n",
        "dense_layer2 = Dense(units=64, activation='relu')(dense_layer1)\n",
        "dense_layer2 = Dropout(0.4)(dense_layer2)\n",
        "\n",
        "output_layer = Dense(units=num_of_classes, activation='softmax')(dense_layer2)\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n"
      ],
      "id": "ECz14lloCVDb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIYUAg_3hOTV"
      },
      "outputs": [],
      "source": [],
      "id": "sIYUAg_3hOTV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkJ0kq38Lws-"
      },
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.0005)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0005)\n",
        "\n",
        "model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['acc'])"
      ],
      "id": "jkJ0kq38Lws-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9ebjkEPL0WA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import History \n",
        "history = History()\n",
        "\n",
        "epochs = 15\n",
        "\n",
        "# let us create checkpoint so that weights are saved after every epoch\n",
        "checkpoint_filepath = ROOT_PATH + \"/\" + \"ccn_checkpoint_6.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_filepath, save_weights_only=True)"
      ],
      "id": "e9ebjkEPL0WA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSHix3Y3PUqY",
        "outputId": "215aad4a-2474-4517-e01f-b3cbf74772fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "21/21 [==============================] - 113s 5s/step - loss: 1.5861 - acc: 0.1949 - val_loss: 1.2519 - val_acc: 0.3594 - lr: 5.0000e-04\n",
            "Epoch 2/15\n",
            "21/21 [==============================] - 91s 5s/step - loss: 1.5610 - acc: 0.2202 - val_loss: 1.2135 - val_acc: 0.1875 - lr: 5.0000e-04\n",
            "Epoch 3/15\n",
            "21/21 [==============================] - 91s 5s/step - loss: 1.4916 - acc: 0.2946 - val_loss: 1.3017 - val_acc: 0.1641 - lr: 5.0000e-04\n",
            "Epoch 4/15\n",
            "21/21 [==============================] - 90s 4s/step - loss: 1.6233 - acc: 0.1905 - val_loss: 1.2571 - val_acc: 0.1719 - lr: 5.0000e-04\n",
            "Epoch 5/15\n",
            "21/21 [==============================] - 91s 5s/step - loss: 1.5921 - acc: 0.2009 - val_loss: 1.2501 - val_acc: 0.1953 - lr: 5.0000e-04\n",
            "Epoch 6/15\n",
            "21/21 [==============================] - 91s 5s/step - loss: 1.5603 - acc: 0.3095 - val_loss: 1.1862 - val_acc: 0.2969 - lr: 5.0000e-04\n",
            "Epoch 7/15\n",
            "21/21 [==============================] - 91s 5s/step - loss: 1.4643 - acc: 0.3393 - val_loss: 1.0800 - val_acc: 0.2969 - lr: 5.0000e-04\n",
            "Epoch 8/15\n",
            "21/21 [==============================] - 89s 4s/step - loss: 1.3856 - acc: 0.3512 - val_loss: 1.2184 - val_acc: 0.2266 - lr: 5.0000e-04\n",
            "Epoch 9/15\n",
            "21/21 [==============================] - 91s 5s/step - loss: 1.3838 - acc: 0.3542 - val_loss: 1.0975 - val_acc: 0.2891 - lr: 5.0000e-04\n",
            "Epoch 10/15\n",
            "21/21 [==============================] - 92s 5s/step - loss: 1.3126 - acc: 0.3631 - val_loss: 1.0660 - val_acc: 0.3047 - lr: 5.0000e-04\n",
            "Epoch 11/15\n",
            "21/21 [==============================] - 91s 5s/step - loss: 1.2598 - acc: 0.3958 - val_loss: 1.1070 - val_acc: 0.2812 - lr: 5.0000e-04\n",
            "Epoch 12/15\n",
            "21/21 [==============================] - 87s 4s/step - loss: 1.2323 - acc: 0.4479 - val_loss: 1.0546 - val_acc: 0.2891 - lr: 5.0000e-04\n",
            "Epoch 13/15\n",
            "21/21 [==============================] - 89s 4s/step - loss: 1.2053 - acc: 0.4494 - val_loss: 1.2847 - val_acc: 0.2188 - lr: 5.0000e-04\n",
            "Epoch 14/15\n",
            "21/21 [==============================] - 90s 4s/step - loss: 1.2519 - acc: 0.4271 - val_loss: 1.3112 - val_acc: 0.1641 - lr: 5.0000e-04\n",
            "Epoch 15/15\n",
            "21/21 [==============================] - 89s 4s/step - loss: 1.4630 - acc: 0.3304 - val_loss: 1.0705 - val_acc: 0.3672 - lr: 5.0000e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f90063fe440>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#with tensorflow.device('/CPU:0'):\n",
        "model.fit(train_batch_generator, validation_data=val_batch_generator, \n",
        "                steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=epochs, \n",
        "                verbose=1, callbacks=[checkpoint, history, reduce_lr])"
      ],
      "id": "QSHix3Y3PUqY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4E4AS4jYV1zZ"
      },
      "outputs": [],
      "source": [],
      "id": "4E4AS4jYV1zZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbC8SpIDUtFq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import History \n",
        "history = History()\n",
        "\n",
        "epochs = 25\n",
        "\n",
        "# let us create checkpoint so that weights are saved after every epoch\n",
        "checkpoint_filepath = ROOT_PATH + \"/\" + \"ccn_checkpoint_7.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_filepath, save_weights_only=True)"
      ],
      "id": "RbC8SpIDUtFq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de2ztgpTVZbT"
      },
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.01)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['acc'])"
      ],
      "id": "de2ztgpTVZbT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgaSH1gqV29n",
        "outputId": "d9673b78-9ae4-423d-8d71-582bdda69c3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "21/21 [==============================] - 49s 2s/step - loss: 1.6234 - acc: 0.1994 - val_loss: 1.6398 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 2/25\n",
            "21/21 [==============================] - 45s 2s/step - loss: 1.5818 - acc: 0.1667 - val_loss: 1.3091 - val_acc: 0.1562 - lr: 0.0100\n",
            "Epoch 3/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.6253 - acc: 0.1756 - val_loss: 1.3005 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 4/25\n",
            "21/21 [==============================] - 47s 2s/step - loss: 1.5753 - acc: 0.1488 - val_loss: 1.6102 - val_acc: 0.2188 - lr: 0.0100\n",
            "Epoch 5/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.6116 - acc: 0.2054 - val_loss: 1.3245 - val_acc: 0.2969 - lr: 0.0100\n",
            "Epoch 6/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.5865 - acc: 0.1488 - val_loss: 1.2981 - val_acc: 0.2656 - lr: 0.0100\n",
            "Epoch 7/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.6120 - acc: 0.1935 - val_loss: 1.6127 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 8/25\n",
            "21/21 [==============================] - 47s 2s/step - loss: 1.5708 - acc: 0.1637 - val_loss: 1.6116 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 9/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.6109 - acc: 0.2411 - val_loss: 1.3010 - val_acc: 0.2500 - lr: 0.0100\n",
            "Epoch 10/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.5726 - acc: 0.1786 - val_loss: 1.3046 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 11/25\n",
            "21/21 [==============================] - 47s 2s/step - loss: 1.6108 - acc: 0.1875 - val_loss: 1.6102 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 12/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.5684 - acc: 0.1905 - val_loss: 1.3074 - val_acc: 0.2969 - lr: 0.0100\n",
            "Epoch 13/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.6093 - acc: 0.1696 - val_loss: 1.3017 - val_acc: 0.2656 - lr: 0.0100\n",
            "Epoch 14/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.5692 - acc: 0.1786 - val_loss: 1.6094 - val_acc: 0.2031 - lr: 0.0100\n",
            "Epoch 15/25\n",
            "21/21 [==============================] - 47s 2s/step - loss: 1.6098 - acc: 0.1845 - val_loss: 1.6114 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 16/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.5686 - acc: 0.1756 - val_loss: 1.3035 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 17/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.6099 - acc: 0.1964 - val_loss: 1.3033 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 18/25\n",
            "21/21 [==============================] - 47s 2s/step - loss: 1.5682 - acc: 0.1696 - val_loss: 1.6087 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 19/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.6095 - acc: 0.1696 - val_loss: 1.3066 - val_acc: 0.2031 - lr: 0.0100\n",
            "Epoch 20/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.5688 - acc: 0.1756 - val_loss: 1.3019 - val_acc: 0.3281 - lr: 0.0100\n",
            "Epoch 21/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.6087 - acc: 0.1994 - val_loss: 1.6102 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 22/25\n",
            "21/21 [==============================] - 47s 2s/step - loss: 1.5680 - acc: 0.1845 - val_loss: 1.6110 - val_acc: 0.2188 - lr: 0.0100\n",
            "Epoch 23/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.6102 - acc: 0.2202 - val_loss: 1.3026 - val_acc: 0.2500 - lr: 0.0100\n",
            "Epoch 24/25\n",
            "21/21 [==============================] - 46s 2s/step - loss: 1.5752 - acc: 0.1905 - val_loss: 1.3036 - val_acc: 0.3750 - lr: 0.0100\n",
            "Epoch 25/25\n",
            "21/21 [==============================] - 47s 2s/step - loss: 1.6114 - acc: 0.1726 - val_loss: 1.6098 - val_acc: 0.1719 - lr: 0.0100\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8f221b4550>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#with tensorflow.device('/CPU:0'):\n",
        "model.fit(train_batch_generator, validation_data=val_batch_generator, \n",
        "                steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=epochs, \n",
        "                verbose=1, callbacks=[checkpoint, history, reduce_lr])"
      ],
      "id": "NgaSH1gqV29n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uK-Gu1_hVf_"
      },
      "outputs": [],
      "source": [
        "# we have 30 images, but we chose 16 below and we pick them randomly and sort to ensure temporal dependence\n",
        "num_samples = 24\n",
        "num_of_channels = 3\n",
        "num_of_classes = 5\n",
        "batch_size =16\n",
        "\n",
        "img_width, img_height = 120, 160\n",
        "\n",
        "# Let us try video classification using conv2D model + RNN - another method\n",
        "\n",
        "input_size_tuple = (num_samples, img_width, img_height, num_of_channels)\n",
        "\n",
        "from keras.layers import Conv2D, BatchNormalization, MaxPool2D, GlobalMaxPool2D\n",
        "\n",
        "def build_convnet(shape=(img_width, img_height, num_of_channels)):\n",
        "    momentum = .9\n",
        "    model = keras.Sequential()\n",
        "    model.add(Conv2D(64, (3,3), input_shape=shape,\n",
        "        padding='same', activation='relu'))\n",
        "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "    #model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(MaxPool2D())\n",
        "    \n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    #model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(MaxPool2D())\n",
        "    \n",
        "    model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
        "    #model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(MaxPool2D())\n",
        "    \n",
        "    model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
        "    #model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    # flatten...\n",
        "    model.add(GlobalMaxPool2D())\n",
        "    return model\n"
      ],
      "id": "1uK-Gu1_hVf_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlGGTPrqV0LU"
      },
      "outputs": [],
      "source": [
        "from keras.layers import TimeDistributed, GRU, Dense, Dropout\n",
        "\n",
        "def action_model(num_of_classes, shape=(num_samples, img_width, img_height, num_of_channels)):\n",
        "    # Create our convnet with (img_width, img_height, 3) input shape\n",
        "    convnet = build_convnet(shape[1:])\n",
        "    \n",
        "    # then create our final model\n",
        "    model = keras.Sequential()\n",
        "    # add the convnet with (number_of_samples, img_widht, img_height, num_of_channels) shape\n",
        "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
        "    # here, you can also use GRU or LSTM\n",
        "    model.add(GRU(64))\n",
        "    # and finally, we make a decision network\n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_of_classes, activation='softmax'))\n",
        "    return model"
      ],
      "id": "SlGGTPrqV0LU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BxqjYA4MZrC6",
        "outputId": "7f94f7fe-1c85-4be5-e5e5-3fb78ae1bd0e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\noptimizer = keras.optimizers.Adam(0.001)\\nmodel.compile(\\n    optimizer,\\n    'categorical_crossentropy',\\n    metrics=['acc']\\n)\\n\""
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "model = action_model(num_of_classes, shape=(num_samples, img_width, img_height, num_of_channels))\n",
        "\n",
        "'''\n",
        "optimizer = keras.optimizers.Adam(0.001)\n",
        "model.compile(\n",
        "    optimizer,\n",
        "    'categorical_crossentropy',\n",
        "    metrics=['acc']\n",
        ")\n",
        "'''"
      ],
      "id": "BxqjYA4MZrC6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXCVIPGQaPGo",
        "outputId": "f9256a3c-1f3a-4984-ffd0-8d8eaa56cb07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " time_distributed_4 (TimeDis  (None, 24, 512)          4685376   \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " gru_4 (GRU)                 (None, 64)                110976    \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1024)              66560     \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 128)               65664     \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,461,957\n",
            "Trainable params: 5,461,957\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ],
      "id": "cXCVIPGQaPGo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iW3qG2bdvJ7"
      },
      "source": [],
      "id": "_iW3qG2bdvJ7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPVLN5Mzais5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import History \n",
        "history = History()\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "# let us create checkpoint so that weights are saved after every epoch\n",
        "checkpoint_filepath = ROOT_PATH + \"/\" + \"ccn_checkpoint_8.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_filepath, save_weights_only=False)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.01)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['acc'])\n"
      ],
      "id": "nPVLN5Mzais5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9B53emyd0Ne"
      },
      "outputs": [],
      "source": [
        "train_batch_generator =  batch_generator(new_train_map_df, batch_size, 'train', train_image_dict)\n",
        "val_batch_generator =  batch_generator(new_val_map_df, batch_size, 'val', val_image_dict)"
      ],
      "id": "G9B53emyd0Ne"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYWnpNXoc4wy",
        "outputId": "63578f01-2fd8-4542-eb85-32aa69836195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "21/21 [==============================] - 58s 2s/step - loss: 1.6808 - acc: 0.2113 - val_loss: 1.6467 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 2/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.5875 - acc: 0.1607 - val_loss: 1.3149 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 3/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.6198 - acc: 0.1756 - val_loss: 1.2997 - val_acc: 0.3750 - lr: 0.0100\n",
            "Epoch 4/20\n",
            "21/21 [==============================] - 43s 2s/step - loss: 1.5761 - acc: 0.2054 - val_loss: 1.6119 - val_acc: 0.2188 - lr: 0.0100\n",
            "Epoch 5/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.6158 - acc: 0.2232 - val_loss: 1.3088 - val_acc: 0.2969 - lr: 0.0100\n",
            "Epoch 6/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.5719 - acc: 0.1488 - val_loss: 1.3064 - val_acc: 0.3281 - lr: 0.0100\n",
            "Epoch 7/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.6120 - acc: 0.1577 - val_loss: 1.6101 - val_acc: 0.2031 - lr: 0.0100\n",
            "Epoch 8/20\n",
            "21/21 [==============================] - 43s 2s/step - loss: 1.5672 - acc: 0.1935 - val_loss: 1.6108 - val_acc: 0.2188 - lr: 0.0100\n",
            "Epoch 9/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.6138 - acc: 0.1875 - val_loss: 1.3011 - val_acc: 0.2500 - lr: 0.0100\n",
            "Epoch 10/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.5711 - acc: 0.1935 - val_loss: 1.3034 - val_acc: 0.3750 - lr: 0.0100\n",
            "Epoch 11/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.6072 - acc: 0.2143 - val_loss: 1.6117 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 12/20\n",
            "21/21 [==============================] - 41s 2s/step - loss: 1.5682 - acc: 0.1667 - val_loss: 1.3114 - val_acc: 0.2969 - lr: 0.0100\n",
            "Epoch 13/20\n",
            "21/21 [==============================] - 41s 2s/step - loss: 1.6178 - acc: 0.1905 - val_loss: 1.2907 - val_acc: 0.2656 - lr: 0.0100\n",
            "Epoch 14/20\n",
            "21/21 [==============================] - 41s 2s/step - loss: 1.5747 - acc: 0.1786 - val_loss: 1.6067 - val_acc: 0.2500 - lr: 0.0100\n",
            "Epoch 15/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.6233 - acc: 0.1756 - val_loss: 1.6127 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 16/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.5844 - acc: 0.1815 - val_loss: 1.3040 - val_acc: 0.3125 - lr: 0.0100\n",
            "Epoch 17/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.6161 - acc: 0.2262 - val_loss: 1.3050 - val_acc: 0.3750 - lr: 0.0100\n",
            "Epoch 18/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.5717 - acc: 0.1994 - val_loss: 1.6065 - val_acc: 0.2188 - lr: 0.0100\n",
            "Epoch 19/20\n",
            "21/21 [==============================] - 41s 2s/step - loss: 1.6354 - acc: 0.1875 - val_loss: 1.3069 - val_acc: 0.2031 - lr: 0.0100\n",
            "Epoch 20/20\n",
            "21/21 [==============================] - 41s 2s/step - loss: 1.5747 - acc: 0.2143 - val_loss: 1.3036 - val_acc: 0.1562 - lr: 0.0100\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f44bc6f1cc0>"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_batch_generator, validation_data=val_batch_generator, \n",
        "          steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=epochs, verbose=1, callbacks=[checkpoint, history, reduce_lr])"
      ],
      "id": "ZYWnpNXoc4wy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2DWo4exnDUL",
        "outputId": "09ec0c21-78df-4daf-d961-079cc7ee2842"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " time_distributed_4 (TimeDis  (None, 24, 512)          4685376   \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " gru_4 (GRU)                 (None, 64)                110976    \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1024)              66560     \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 128)               65664     \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,461,957\n",
            "Trainable params: 5,461,957\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "loaded_model = keras.models.load_model(checkpoint_filepath)\n",
        "loaded_model.summary()"
      ],
      "id": "e2DWo4exnDUL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JEuQdXnnTB7",
        "outputId": "fe1e45b4-00a2-4eeb-93ad-469064c99e4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "21/21 [==============================] - 43s 2s/step - loss: 1.6131 - acc: 0.1726 - val_loss: 1.6084 - val_acc: 0.2031 - lr: 0.0100\n",
            "Epoch 2/20\n",
            "21/21 [==============================] - 43s 2s/step - loss: 1.5831 - acc: 0.1667 - val_loss: 1.6110 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 3/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.6174 - acc: 0.1994 - val_loss: 1.2952 - val_acc: 0.2500 - lr: 0.0100\n",
            "Epoch 4/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.5701 - acc: 0.2054 - val_loss: 1.3056 - val_acc: 0.1562 - lr: 0.0100\n",
            "Epoch 5/20\n",
            "21/21 [==============================] - 43s 2s/step - loss: 1.6124 - acc: 0.1935 - val_loss: 1.6084 - val_acc: 0.2188 - lr: 0.0100\n",
            "Epoch 6/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.5678 - acc: 0.1964 - val_loss: 1.3062 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 7/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.6105 - acc: 0.1756 - val_loss: 1.3016 - val_acc: 0.2656 - lr: 0.0100\n",
            "Epoch 8/20\n",
            "21/21 [==============================] - 43s 2s/step - loss: 1.5680 - acc: 0.1548 - val_loss: 1.6093 - val_acc: 0.2031 - lr: 0.0100\n",
            "Epoch 9/20\n",
            "21/21 [==============================] - 43s 2s/step - loss: 1.6099 - acc: 0.1756 - val_loss: 1.6119 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 10/20\n",
            "21/21 [==============================] - 41s 2s/step - loss: 1.5682 - acc: 0.1726 - val_loss: 1.3037 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 11/20\n",
            "21/21 [==============================] - 43s 2s/step - loss: 1.6094 - acc: 0.1786 - val_loss: 1.3029 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 12/20\n",
            "21/21 [==============================] - 43s 2s/step - loss: 1.5687 - acc: 0.1756 - val_loss: 1.6085 - val_acc: 0.2188 - lr: 0.0100\n",
            "Epoch 13/20\n",
            "21/21 [==============================] - 43s 2s/step - loss: 1.6102 - acc: 0.1756 - val_loss: 1.3064 - val_acc: 0.2031 - lr: 0.0100\n",
            "Epoch 14/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.5675 - acc: 0.1756 - val_loss: 1.3035 - val_acc: 0.1562 - lr: 0.0100\n",
            "Epoch 15/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.6096 - acc: 0.1905 - val_loss: 1.6106 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 16/20\n",
            "21/21 [==============================] - 43s 2s/step - loss: 1.5682 - acc: 0.1815 - val_loss: 1.6108 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 17/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.6092 - acc: 0.2083 - val_loss: 1.3005 - val_acc: 0.2500 - lr: 0.0100\n",
            "Epoch 18/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.5700 - acc: 0.1726 - val_loss: 1.3049 - val_acc: 0.3750 - lr: 0.0100\n",
            "Epoch 19/20\n",
            "21/21 [==============================] - 43s 2s/step - loss: 1.6125 - acc: 0.2232 - val_loss: 1.6158 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 20/20\n",
            "21/21 [==============================] - 42s 2s/step - loss: 1.5723 - acc: 0.2083 - val_loss: 1.3052 - val_acc: 0.1875 - lr: 0.0100\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f44bc6f1cc0>"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epochs = 20\n",
        "loaded_model.fit(train_batch_generator, validation_data=val_batch_generator, \n",
        "          steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=epochs, verbose=1, callbacks=[checkpoint, history, reduce_lr])"
      ],
      "id": "0JEuQdXnnTB7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tHib7KYsAea"
      },
      "outputs": [],
      "source": [
        "# we have 30 images, but we chose 12 below and we pick them randomly and sort to ensure temporal dependence\n",
        "num_samples = 12\n",
        "num_of_channels = 3\n",
        "num_of_classes = 5\n",
        "batch_size =16\n",
        "\n",
        "img_width, img_height = 120, 160\n",
        "\n",
        "# Let us try video classification using conv2D model + RNN - another method\n",
        "\n",
        "input_size_tuple = (num_samples, img_width, img_height, num_of_channels)\n",
        "\n",
        "from keras.layers import Conv2D, BatchNormalization, MaxPool2D, GlobalMaxPool2D\n",
        "\n",
        "def build_convnet(shape=(img_width, img_height, num_of_channels)):\n",
        "    momentum = .9\n",
        "    model = keras.Sequential()\n",
        "    model.add(Conv2D(64, (3,3), input_shape=shape,\n",
        "        padding='same', activation='relu'))\n",
        "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "    #model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(MaxPool2D())\n",
        "    \n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    #model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(MaxPool2D())\n",
        "    \n",
        "    model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(256, (3,3), padding='same', activation='relu'))\n",
        "    #model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(MaxPool2D())\n",
        "    \n",
        "    model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(512, (3,3), padding='same', activation='relu'))\n",
        "    #model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    # flatten...\n",
        "    model.add(GlobalMaxPool2D())\n",
        "    return model"
      ],
      "id": "3tHib7KYsAea"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wyn0Z1_idKIi"
      },
      "id": "Wyn0Z1_idKIi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwgxHVTfsOFD"
      },
      "outputs": [],
      "source": [
        "from keras.layers import TimeDistributed, GRU, Dense, Dropout\n",
        "\n",
        "def action_model(num_of_classes, shape=(num_samples, img_width, img_height, num_of_channels)):\n",
        "    # Create our convnet with (img_width, img_height, 3) input shape\n",
        "    convnet = build_convnet(shape[1:])\n",
        "    \n",
        "    # then create our final model\n",
        "    model = keras.Sequential()\n",
        "    # add the convnet with (number_of_samples, img_widht, img_height, num_of_channels) shape\n",
        "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
        "    # here, you can also use GRU or LSTM\n",
        "    model.add(GRU(64))\n",
        "    # and finally, we make a decision network\n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_of_classes, activation='softmax'))\n",
        "    return model"
      ],
      "id": "WwgxHVTfsOFD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lntXWyQbsYlo",
        "outputId": "52d07fd2-a2f3-4e1d-afb6-dd8c285dfc28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " time_distributed_4 (TimeDis  (None, 24, 512)          4685376   \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " gru_4 (GRU)                 (None, 64)                110976    \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1024)              66560     \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 128)               65664     \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,461,957\n",
            "Trainable params: 5,461,957\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ],
      "id": "lntXWyQbsYlo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ4TAImDscMk"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import History \n",
        "history = History()\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "# let us create checkpoint so that weights are saved after every epoch\n",
        "checkpoint_filepath = ROOT_PATH + \"/\" + \"ccn_checkpoint_9.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_filepath, save_weights_only=False)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.01)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['acc'])"
      ],
      "id": "pJ4TAImDscMk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnXOW3GPsr7e",
        "outputId": "b9eefa8a-10aa-47c5-f7f9-cf4fe63df478"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "21/21 [==============================] - 37s 1s/step - loss: 1.6195 - acc: 0.2054 - val_loss: 1.6131 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 2/20\n",
            "21/21 [==============================] - 28s 1s/step - loss: 1.5707 - acc: 0.1964 - val_loss: 1.3034 - val_acc: 0.3125 - lr: 0.0100\n",
            "Epoch 3/20\n",
            "21/21 [==============================] - 28s 1s/step - loss: 1.6107 - acc: 0.1607 - val_loss: 1.2999 - val_acc: 0.3750 - lr: 0.0100\n",
            "Epoch 4/20\n",
            "21/21 [==============================] - 29s 1s/step - loss: 1.5732 - acc: 0.1964 - val_loss: 1.6083 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 5/20\n",
            "21/21 [==============================] - 28s 1s/step - loss: 1.6261 - acc: 0.1548 - val_loss: 1.3055 - val_acc: 0.2031 - lr: 0.0100\n",
            "Epoch 6/20\n",
            "21/21 [==============================] - 29s 1s/step - loss: 1.5742 - acc: 0.1577 - val_loss: 1.3018 - val_acc: 0.2656 - lr: 0.0100\n",
            "Epoch 7/20\n",
            "21/21 [==============================] - 29s 1s/step - loss: 1.6131 - acc: 0.1815 - val_loss: 1.6105 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 8/20\n",
            "21/21 [==============================] - 29s 1s/step - loss: 1.5762 - acc: 0.1935 - val_loss: 1.6112 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 9/20\n",
            "21/21 [==============================] - 29s 1s/step - loss: 1.6124 - acc: 0.1845 - val_loss: 1.3007 - val_acc: 0.3125 - lr: 0.0100\n",
            "Epoch 10/20\n",
            "21/21 [==============================] - 29s 1s/step - loss: 1.5805 - acc: 0.1607 - val_loss: 1.3037 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 11/20\n",
            "21/21 [==============================] - 29s 1s/step - loss: 1.6244 - acc: 0.2113 - val_loss: 1.6058 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 12/20\n",
            "21/21 [==============================] - 29s 1s/step - loss: 1.5789 - acc: 0.1875 - val_loss: 1.3051 - val_acc: 0.2031 - lr: 0.0100\n",
            "Epoch 13/20\n",
            "21/21 [==============================] - 29s 1s/step - loss: 1.6186 - acc: 0.1815 - val_loss: 1.2992 - val_acc: 0.2656 - lr: 0.0100\n",
            "Epoch 14/20\n",
            "21/21 [==============================] - 28s 1s/step - loss: 1.5776 - acc: 0.1845 - val_loss: 1.6090 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 15/20\n",
            "21/21 [==============================] - 29s 1s/step - loss: 1.6129 - acc: 0.1786 - val_loss: 1.6113 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 16/20\n",
            "21/21 [==============================] - 29s 1s/step - loss: 1.5698 - acc: 0.1815 - val_loss: 1.3024 - val_acc: 0.2500 - lr: 0.0100\n",
            "Epoch 17/20\n",
            "21/21 [==============================] - 29s 1s/step - loss: 1.6115 - acc: 0.1875 - val_loss: 1.3033 - val_acc: 0.1875 - lr: 0.0100\n",
            "Epoch 18/20\n",
            "21/21 [==============================] - 29s 1s/step - loss: 1.5655 - acc: 0.1815 - val_loss: 1.6082 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 19/20\n",
            "21/21 [==============================] - 29s 1s/step - loss: 1.6166 - acc: 0.1756 - val_loss: 1.3060 - val_acc: 0.2031 - lr: 0.0100\n",
            "Epoch 20/20\n",
            "21/21 [==============================] - 29s 1s/step - loss: 1.5757 - acc: 0.1875 - val_loss: 1.3039 - val_acc: 0.1406 - lr: 0.0100\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f44d01cb460>"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(train_batch_generator, validation_data=val_batch_generator, \n",
        "          steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=epochs, verbose=1, callbacks=[checkpoint, history, reduce_lr])"
      ],
      "id": "LnXOW3GPsr7e"
    },
    {
      "cell_type": "code",
      "source": [
        "# we have 30 images, but we chose 28 images below and we pick them randomly and sort to ensure temporal dependence\n",
        "num_samples = 28\n",
        "num_of_channels = 3\n",
        "num_of_classes = 5\n",
        "batch_size =16\n",
        "\n",
        "img_width, img_height = 120, 160\n",
        "\n",
        "# Let us try video classification using conv2D model + RNN - another method\n",
        "\n",
        "input_size_tuple = (num_samples, img_width, img_height, num_of_channels)\n",
        "\n",
        "from keras.layers import Conv2D, BatchNormalization, MaxPool2D, GlobalMaxPool2D\n",
        "\n",
        "def build_convnet(shape=(img_width, img_height, num_of_channels)):\n",
        "    momentum = .9\n",
        "    model = keras.Sequential()\n",
        "    model.add(Conv2D(16, (3,3), input_shape=shape,\n",
        "        padding='same', activation='relu'))\n",
        "    model.add(Conv2D(16, (3,3), padding='same', activation='relu'))\n",
        "    #model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(MaxPool2D())\n",
        "    \n",
        "    model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n",
        "    #model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(MaxPool2D())\n",
        "    \n",
        "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "    #model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(MaxPool2D())\n",
        "    \n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    #model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    # flatten...\n",
        "    model.add(GlobalMaxPool2D())\n",
        "    return model"
      ],
      "metadata": {
        "id": "Ky-7P0qldLnh"
      },
      "id": "Ky-7P0qldLnh",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import TimeDistributed, GRU, Dense, Dropout\n",
        "\n",
        "def action_model(num_of_classes, shape=(num_samples, img_width, img_height, num_of_channels)):\n",
        "    # Create our convnet with (img_width, img_height, 3) input shape\n",
        "    convnet = build_convnet(shape[1:])\n",
        "    \n",
        "    # then create our final model\n",
        "    model = keras.Sequential()\n",
        "    # add the convnet with (number_of_samples, img_widht, img_height, num_of_channels) shape\n",
        "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
        "    # here, you can also use GRU or LSTM\n",
        "    model.add(GRU(64))\n",
        "    # and finally, we make a decision network\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(.2))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(num_of_classes, activation='softmax'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "6BIa5gGbdkYP"
      },
      "id": "6BIa5gGbdkYP",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = action_model(num_of_classes, shape=(num_samples, img_width, img_height, num_of_channels))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrlGQ4AidoEf",
        "outputId": "918c7aa3-bb1f-40fc-bde0-4552e693dbae"
      },
      "id": "JrlGQ4AidoEf",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " time_distributed_1 (TimeDis  (None, 28, 128)          293520    \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 64)                37248     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               8320      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 5)                 85        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 350,037\n",
            "Trainable params: 350,037\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import History \n",
        "history = History()\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "# let us create checkpoint so that weights are saved after every epoch\n",
        "checkpoint_filepath = ROOT_PATH + \"/\" + \"ccn_checkpoint_10.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_filepath, save_weights_only=False)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.01)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['acc'])"
      ],
      "metadata": {
        "id": "b3RTojaJdpoH"
      },
      "id": "b3RTojaJdpoH",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_batch_generator, validation_data=val_batch_generator, \n",
        "          steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=epochs, verbose=1, callbacks=[checkpoint, history, reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8ZLuVlSeaN8",
        "outputId": "717c7240-f1d2-4c2c-bc3d-f8cac2cd8756"
      },
      "id": "t8ZLuVlSeaN8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "21/21 [==============================] - 105s 4s/step - loss: 1.6507 - acc: 0.1905 - val_loss: 1.2553 - val_acc: 0.1641 - lr: 0.0100\n",
            "Epoch 2/20\n",
            "21/21 [==============================] - 87s 4s/step - loss: 1.5986 - acc: 0.1741 - val_loss: 1.2547 - val_acc: 0.1641 - lr: 0.0100\n",
            "Epoch 3/20\n",
            "21/21 [==============================] - 88s 4s/step - loss: 1.5885 - acc: 0.2188 - val_loss: 1.2547 - val_acc: 0.1641 - lr: 0.0100\n",
            "Epoch 4/20\n",
            "21/21 [==============================] - 86s 4s/step - loss: 1.5904 - acc: 0.2202 - val_loss: 1.2564 - val_acc: 0.1719 - lr: 0.0100\n",
            "Epoch 5/20\n",
            "16/21 [=====================>........] - ETA: 18s - loss: 1.6149 - acc: 0.2168"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we have 30 images, but we chose 28 images below and we pick them randomly and sort to ensure temporal dependence\n",
        "num_samples = 28\n",
        "num_of_channels = 3\n",
        "num_of_classes = 5\n",
        "batch_size = 32\n",
        "\n",
        "img_width, img_height = 120, 160\n",
        "\n",
        "# Let us try video classification using conv2D model + RNN - another method\n",
        "\n",
        "input_size_tuple = (num_samples, img_width, img_height, num_of_channels)\n",
        "\n",
        "from keras.layers import Conv2D, BatchNormalization, MaxPool2D, GlobalMaxPool2D\n",
        "\n",
        "def build_convnet(shape=(img_width, img_height, num_of_channels)):\n",
        "    momentum = .9\n",
        "    model = keras.Sequential()\n",
        "    model.add(Conv2D(32, (3,3), input_shape=shape,\n",
        "        padding='same', activation='relu'))\n",
        "    model.add(Conv2D(32, (3,3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    model.add(MaxPool2D())\n",
        "    \n",
        "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "\n",
        "    model.add(MaxPool2D())\n",
        "    \n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization(momentum=momentum))\n",
        "    \n",
        "    # flatten...\n",
        "    model.add(GlobalMaxPool2D())\n",
        "\n",
        "    '''\n",
        "    model.add(MaxPool2D())\n",
        "    \n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "    #model.add(BatchNormalization(momentum=momentum))\n",
        "  \n",
        "    # flatten...\n",
        "    model.add(GlobalMaxPool2D())\n",
        "    '''\n",
        "    return model"
      ],
      "metadata": {
        "id": "lHNj94sTnw_D"
      },
      "id": "lHNj94sTnw_D",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import TimeDistributed, GRU, Dense, Dropout\n",
        "\n",
        "def action_model(num_of_classes, shape=(num_samples, img_width, img_height, num_of_channels)):\n",
        "    # Create our convnet with (img_width, img_height, 3) input shape\n",
        "    convnet = build_convnet(shape[1:])\n",
        "    \n",
        "    # then create our final model\n",
        "    model = keras.Sequential()\n",
        "    # add the convnet with (number_of_samples, img_widht, img_height, num_of_channels) shape\n",
        "    model.add(TimeDistributed(convnet, input_shape=shape))\n",
        "    # here, you can also use GRU or LSTM\n",
        "    model.add(GRU(64))\n",
        "    # and finally, we make a decision network\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(.3))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(.2))\n",
        "    #model.add(Dense(32, activation='relu'))\n",
        "    #model.add(Dropout(.2))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(num_of_classes, activation='softmax'))\n",
        "    return model"
      ],
      "metadata": {
        "id": "qU6z-LRMn0Pj"
      },
      "id": "qU6z-LRMn0Pj",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = action_model(num_of_classes, shape=(num_samples, img_width, img_height, num_of_channels))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2shwiabn3Wb",
        "outputId": "4e1a669a-19a1-42ca-9dc6-2f838db4669a"
      },
      "id": "T2shwiabn3Wb",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " time_distributed (TimeDistr  (None, 28, 128)          287904    \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 64)                37248     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                4160      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 85        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 332,005\n",
            "Trainable params: 331,557\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c_TsLHnnn8dz"
      },
      "id": "c_TsLHnnn8dz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import History \n",
        "history = History()\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "# let us create checkpoint so that weights are saved after every epoch\n",
        "checkpoint_filepath = ROOT_PATH + \"/\" + \"ccn_checkpoint_11.h5\"\n",
        "checkpoint = ModelCheckpoint(checkpoint_filepath, save_weights_only=False)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=5, min_lr=0.1)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.1)\n",
        "\n",
        "model.compile(loss=categorical_crossentropy, optimizer=opt, metrics=['acc'])"
      ],
      "metadata": {
        "id": "iQAbILACn5zl"
      },
      "id": "iQAbILACn5zl",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_generator =  batch_generator(new_train_map_df, batch_size, 'train', train_image_dict)\n",
        "val_batch_generator =  batch_generator(new_val_map_df, batch_size, 'val', val_image_dict)"
      ],
      "metadata": {
        "id": "Xcf1xUSToN5t"
      },
      "id": "Xcf1xUSToN5t",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_batch_generator, validation_data=val_batch_generator, \n",
        "          steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=epochs, verbose=1, callbacks=[checkpoint, history, reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cW5U1sOkn9dy",
        "outputId": "6dc5d794-ebda-4a35-ac32-d960ed542a65"
      },
      "id": "cW5U1sOkn9dy",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ebd3ca58a518>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(train_batch_generator, validation_data=val_batch_generator, \n\u001b[0m\u001b[1;32m      2\u001b[0m           steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, epochs=epochs, verbose=1, callbacks=[checkpoint, history, reduce_lr])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_1/time_distributed/batch_normalization/FusedBatchNormV3' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n      self.ctx_run(self.run)\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-21-ebd3ca58a518>\", line 1, in <cell line: 1>\n      model.fit(train_batch_generator, validation_data=val_batch_generator,\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1150, in __call__\n      self._set_mask_metadata(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 2884, in _set_mask_metadata\n      output_masks = self.compute_mask(inputs, previous_mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/layers/rnn/time_distributed.py\", line 329, in compute_mask\n      output_mask = self.layer.compute_mask(inner_inputs, inner_mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/sequential.py\", line 447, in compute_mask\n      outputs = self.call(inputs, mask=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/layers/normalization/batch_normalization.py\", line 922, in call\n      outputs = self._fused_batch_norm(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/layers/normalization/batch_normalization.py\", line 688, in _fused_batch_norm\n      output, mean, variance = control_flow_util.smart_cond(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/layers/normalization/batch_normalization.py\", line 662, in _fused_batch_norm_training\n      return tf.compat.v1.nn.fused_batch_norm(\nNode: 'sequential_1/time_distributed/batch_normalization/FusedBatchNormV3'\nOOM when allocating tensor with shape[896,32,120,160] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential_1/time_distributed/batch_normalization/FusedBatchNormV3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_4792]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}